# -*- coding: utf-8 -*-
"""Custom_CIFAR100.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f2SdzGR2kB3k7m6P8bS9FHifG3oGehbb

Import Required Libraries
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torchvision import datasets, transforms

"""Define Data Transformations"""

# Define transformations for the dataset
data_transforms = transforms.Compose([
    transforms.Resize((32, 32)),  # CIFAR100 images are already 32x32
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

"""Load CIFAR100"""

# Define dataset root directory
dataset_root = './data'

# Load CIFAR100 dataset
trainset = datasets.CIFAR100(root=dataset_root, train=True, transform=data_transforms, download=True)
testset = datasets.CIFAR100(root=dataset_root, train=False, transform=data_transforms, download=True)

"""Create Data Loaders"""

# Define data loaders
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)

"""Custom CNN Architecture"""

class CustomCNN(nn.Module):
    def __init__(self, num_classes=100):
        super(CustomCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # Conv Layer 1
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Max Pool Layer 1

            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Conv Layer 2
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2),  # Max Pool Layer 2

            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Conv Layer 3
            nn.ReLU()
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(256 * 8 * 8, 512),  # Fully Connected Layer
            nn.ReLU(),
            nn.Linear(512, num_classes)  # Output Layer
        )

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

# Instantiate the model
model = CustomCNN(num_classes=100)

"""Set Device and Hyperparams"""

# Move model to GPU if available
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Define loss function, optimizer, and learning rate
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

"""Train Model"""

# Training loop
num_epochs = 30
best_acc = 0.0
best_model_wts = model.state_dict()

for epoch in range(num_epochs):
    print(f'Epoch {epoch + 1}/{num_epochs}')
    print('-' * 10)

    model.train()
    running_loss = 0.0
    running_corrects = 0

    for images, labels in trainloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, preds = torch.max(outputs, 1)
        running_loss += loss.item() * images.size(0)
        running_corrects += torch.sum(preds == labels.data)

    epoch_loss = running_loss / len(trainset)
    epoch_acc = running_corrects.double() / len(trainset)
    print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

    if epoch_acc > best_acc:
        best_acc = epoch_acc
        best_model_wts = model.state_dict()

torch.save(best_model_wts, 'best_custom_model.pth')

"""Test Model"""

# Load the best model weights
model.load_state_dict(torch.load('best_custom_model.pth'))
model.eval()

running_corrects = 0

for images, labels in testloader:
    images, labels = images.to(device), labels.to(device)

    outputs = model(images)
    _, preds = torch.max(outputs, 1)
    running_corrects += torch.sum(preds == labels.data)

test_acc = running_corrects.double() / len(testset)
print(f'Test Accuracy: {test_acc:.4f}')