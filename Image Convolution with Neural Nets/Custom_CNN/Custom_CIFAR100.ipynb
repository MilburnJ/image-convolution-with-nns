{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Required Libraries"
      ],
      "metadata": {
        "id": "EMvSm5AJL9C8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R94VoqrWL3JR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Data Transformations"
      ],
      "metadata": {
        "id": "pWK31OvhMClo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for the dataset\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # CIFAR100 images are already 32x32\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n"
      ],
      "metadata": {
        "id": "OiH_98z5MIC6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load CIFAR100"
      ],
      "metadata": {
        "id": "vMfZ2RzKMM-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset root directory\n",
        "dataset_root = './data'\n",
        "\n",
        "# Load CIFAR100 dataset\n",
        "trainset = datasets.CIFAR100(root=dataset_root, train=True, transform=data_transforms, download=True)\n",
        "testset = datasets.CIFAR100(root=dataset_root, train=False, transform=data_transforms, download=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ng6-QbMSzJ",
        "outputId": "acd9a920-f7e1-4b8e-b206-9a7d6d3f0989"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169M/169M [00:13<00:00, 13.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Data Loaders"
      ],
      "metadata": {
        "id": "fAvfsnmRMV-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data loaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "id": "nE2JZJyTMW7I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom CNN Architecture"
      ],
      "metadata": {
        "id": "NnsfM7x-MdO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # Conv Layer 1\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max Pool Layer 1\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Conv Layer 2\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Max Pool Layer 2\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Conv Layer 3\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256 * 8 * 8, 512),  # Fully Connected Layer\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_classes)  # Output Layer\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CustomCNN(num_classes=100)\n"
      ],
      "metadata": {
        "id": "0oKOI38dMgMm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Device and Hyperparams"
      ],
      "metadata": {
        "id": "X8TKf7n1Mjjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and learning rate\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "nPcHEOgMMlax"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "V3FiG3dBMn2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 30\n",
        "best_acc = 0.0\n",
        "best_model_wts = model.state_dict()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(trainset)\n",
        "    epoch_acc = running_corrects.double() / len(trainset)\n",
        "    print(f'Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "    if epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        best_model_wts = model.state_dict()\n",
        "\n",
        "torch.save(best_model_wts, 'best_custom_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk1KE0dOMqJq",
        "outputId": "310ce5f0-f3af-4132-fcaa-7f27fb40a6bc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "----------\n",
            "Loss: 3.7336 Acc: 0.1333\n",
            "Epoch 2/30\n",
            "----------\n",
            "Loss: 2.8726 Acc: 0.2838\n",
            "Epoch 3/30\n",
            "----------\n",
            "Loss: 2.4029 Acc: 0.3824\n",
            "Epoch 4/30\n",
            "----------\n",
            "Loss: 2.0304 Acc: 0.4621\n",
            "Epoch 5/30\n",
            "----------\n",
            "Loss: 1.7153 Acc: 0.5364\n",
            "Epoch 6/30\n",
            "----------\n",
            "Loss: 1.4144 Acc: 0.6074\n",
            "Epoch 7/30\n",
            "----------\n",
            "Loss: 1.1319 Acc: 0.6828\n",
            "Epoch 8/30\n",
            "----------\n",
            "Loss: 0.8812 Acc: 0.7490\n",
            "Epoch 9/30\n",
            "----------\n",
            "Loss: 0.6703 Acc: 0.8090\n",
            "Epoch 10/30\n",
            "----------\n",
            "Loss: 0.5096 Acc: 0.8514\n",
            "Epoch 11/30\n",
            "----------\n",
            "Loss: 0.4008 Acc: 0.8815\n",
            "Epoch 12/30\n",
            "----------\n",
            "Loss: 0.3066 Acc: 0.9105\n",
            "Epoch 13/30\n",
            "----------\n",
            "Loss: 0.2525 Acc: 0.9255\n",
            "Epoch 14/30\n",
            "----------\n",
            "Loss: 0.2058 Acc: 0.9384\n",
            "Epoch 15/30\n",
            "----------\n",
            "Loss: 0.1818 Acc: 0.9460\n",
            "Epoch 16/30\n",
            "----------\n",
            "Loss: 0.1629 Acc: 0.9504\n",
            "Epoch 17/30\n",
            "----------\n",
            "Loss: 0.1565 Acc: 0.9521\n",
            "Epoch 18/30\n",
            "----------\n",
            "Loss: 0.1457 Acc: 0.9551\n",
            "Epoch 19/30\n",
            "----------\n",
            "Loss: 0.1323 Acc: 0.9590\n",
            "Epoch 20/30\n",
            "----------\n",
            "Loss: 0.1411 Acc: 0.9566\n",
            "Epoch 21/30\n",
            "----------\n",
            "Loss: 0.1178 Acc: 0.9643\n",
            "Epoch 22/30\n",
            "----------\n",
            "Loss: 0.1076 Acc: 0.9659\n",
            "Epoch 23/30\n",
            "----------\n",
            "Loss: 0.1167 Acc: 0.9640\n",
            "Epoch 24/30\n",
            "----------\n",
            "Loss: 0.1058 Acc: 0.9681\n",
            "Epoch 25/30\n",
            "----------\n",
            "Loss: 0.1084 Acc: 0.9685\n",
            "Epoch 26/30\n",
            "----------\n",
            "Loss: 0.0856 Acc: 0.9741\n",
            "Epoch 27/30\n",
            "----------\n",
            "Loss: 0.0922 Acc: 0.9733\n",
            "Epoch 28/30\n",
            "----------\n",
            "Loss: 0.0906 Acc: 0.9739\n",
            "Epoch 29/30\n",
            "----------\n",
            "Loss: 0.0813 Acc: 0.9754\n",
            "Epoch 30/30\n",
            "----------\n",
            "Loss: 0.0913 Acc: 0.9723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Model"
      ],
      "metadata": {
        "id": "siTShHxXMumC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model weights\n",
        "model.load_state_dict(torch.load('best_custom_model.pth'))\n",
        "model.eval()\n",
        "\n",
        "running_corrects = 0\n",
        "\n",
        "for images, labels in testloader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(images)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "test_acc = running_corrects.double() / len(testset)\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eswpc6KfMsQv",
        "outputId": "c14ccb38-ee9d-45e3-d3da-28f82a087485"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4af2c7bdef62>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_custom_model.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.4569\n"
          ]
        }
      ]
    }
  ]
}